"""
å¾®åšè¯­æ–™åˆ†ææ¨¡å—

æ­¤æ¨¡å—æä¾›æ–‡æœ¬æ¸…æ´—ã€åˆ†è¯ã€è¯é¢‘ç»Ÿè®¡ç­‰è¯­æ–™å¤„ç†åŠŸèƒ½ã€‚
"""

import pandas as pd
import jieba
import re
from collections import Counter
from wordcloud import WordCloud
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.pyplot as plt
import math


def clean_text(text):
    """
    æ¸…æ´—æ–‡æœ¬ï¼Œç§»é™¤URLå’Œç‰¹æ®Šç¬¦å·ã€‚
    
    å‚æ•°ï¼š
    -----------
    text : str or None
        å¾…æ¸…æ´—çš„æ–‡æœ¬
        
    è¿”å›å€¼ï¼š
    -----------
    str
        æ¸…æ´—åçš„æ–‡æœ¬
        
    è¯´æ˜ï¼š
    -----------
    - ç§»é™¤HTTP/HTTPS URL
    - ç§»é™¤è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šç¬¦å·
    - ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’Œæ•°å­—
    
    ç¤ºä¾‹ï¼š
    -----------
    >>> text = "è¿™æ˜¯ä¸€æ¡å¾®åš http://example.com ğŸ˜Š #è¯é¢˜#"
    >>> result = clean_text(text)
    >>> print(result)  # è¾“å‡º: è¿™æ˜¯ä¸€æ¡å¾®åšè¯é¢˜
    """
    if pd.isna(text):
        return ""
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    text = str(text)
    
    # ç§»é™¤URL
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    
    # ç§»é™¤è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šç¬¦å·ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’Œæ•°å­—
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', text)
    
    return text


def tokenize_and_count_words(df, text_column='å¾®åšæ­£æ–‡', keyword_column='å…³é”®è¯', 
                              word_length_range=(2, 4)):
    """
    æŒ‰å…³é”®è¯åˆ†ç»„è¿›è¡Œåˆ†è¯å’Œè¯é¢‘ç»Ÿè®¡ã€‚
    
    å‚æ•°ï¼š
    -----------
    df : pd.DataFrame
        è¾“å…¥DataFrameï¼Œå¿…é¡»åŒ…å«æ–‡æœ¬åˆ—å’Œå…³é”®è¯åˆ—
    text_column : str, optional
        åŒ…å«æ–‡æœ¬çš„åˆ—åï¼ˆé»˜è®¤ä¸º'å¾®åšæ­£æ–‡'ï¼‰
    keyword_column : str, optional
        åŒ…å«å…³é”®è¯çš„åˆ—åï¼ˆé»˜è®¤ä¸º'å…³é”®è¯'ï¼‰
    word_length_range : tuple, optional
        ä¿ç•™è¯çš„é•¿åº¦èŒƒå›´ (æœ€å°é•¿åº¦, æœ€å¤§é•¿åº¦)ï¼Œé»˜è®¤ä¸º(2, 4)
        
    è¿”å›å€¼ï¼š
    -----------
    dict
        é”®ä¸ºå…³é”®è¯ï¼Œå€¼ä¸ºè¯¥å…³é”®è¯ä¸‹çš„è¯é¢‘Counterå¯¹è±¡çš„å­—å…¸
        
    è¯´æ˜ï¼š
    -----------
    - ä½¿ç”¨jiebaåˆ†è¯
    - ä¼šè·³è¿‡ç©ºç™½æ–‡æœ¬è¡Œ
    - è¿”å›çš„æ˜¯å„å…³é”®è¯çš„è¯é¢‘ç»Ÿè®¡å­—å…¸
        
    ç¤ºä¾‹ï¼š
    -----------
    >>> word_freq_by_keyword = tokenize_and_count_words(df)
    >>> print(word_freq_by_keyword['å…³é”®è¯1'].most_common(10))
    """
    # æ¸…æ´—æ–‡æœ¬
    print("å¼€å§‹æ¸…æ´—æ–‡æœ¬...")
    df_clean = df.copy()
    df_clean[f'{text_column}_cleaned'] = df_clean[text_column].apply(clean_text)
    
    # ç§»é™¤ç©ºæ–‡æœ¬
    df_with_text = df_clean[df_clean[f'{text_column}_cleaned'].str.len() > 0].copy()
    print(f"æœ‰æ•ˆæ–‡æœ¬æ•°: {len(df_with_text)}")
    
    # æŒ‰å…³é”®è¯åˆ†åˆ«è¿›è¡Œåˆ†è¯
    print("\nå¼€å§‹æŒ‰å…³é”®è¯åˆ†è¯...")
    
    keywords_list = df_with_text[keyword_column].unique()
    word_freq_by_keyword = {}
    min_len, max_len = word_length_range
    
    for keyword in keywords_list:
        keyword_texts = df_with_text[df_with_text[keyword_column] == keyword][f'{text_column}_cleaned']
        
        all_words = []
        for text in keyword_texts:
            # ä½¿ç”¨jiebaåˆ†è¯
            words = jieba.cut(text)
            # ç­›é€‰æŒ‡å®šé•¿åº¦çš„è¯
            filtered_words = [word for word in words if min_len <= len(word) <= max_len]
            all_words.extend(filtered_words)
        
        # è¯é¢‘ç»Ÿè®¡
        word_freq = Counter(all_words)
        word_freq_by_keyword[keyword] = word_freq
        print(f"  {keyword}: {len(all_words)} è¯ï¼Œ{len(word_freq)} ç‹¬ç‰¹è¯")
    
    return word_freq_by_keyword


def create_word_frequency_dataframe(word_freq_by_keyword, top_n=100):
    """
    å°†è¯é¢‘å­—å…¸è½¬æ¢ä¸ºé•¿æ ¼å¼çš„DataFrameã€‚
    
    å‚æ•°ï¼š
    -----------
    word_freq_by_keyword : dict
        é”®ä¸ºå…³é”®è¯ï¼Œå€¼ä¸ºCounterå¯¹è±¡çš„å­—å…¸
    top_n : int, optional
        æ¯ä¸ªå…³é”®è¯ä¿ç•™çš„å‰Nä¸ªé«˜é¢‘è¯ï¼ˆé»˜è®¤ä¸º100ï¼‰
        
    è¿”å›å€¼ï¼š
    -----------
    pd.DataFrame
        é•¿æ ¼å¼çš„DataFrameï¼ŒåŒ…å«'å…³é”®è¯'ã€'è¯'ã€'è¯é¢‘'ä¸‰åˆ—
        
    è¯´æ˜ï¼š
    -----------
    - æ¯è¡Œä»£è¡¨ä¸€ä¸ª(å…³é”®è¯, è¯)å¯¹åŠå…¶é¢‘ç‡
    - ä»…ä¿ç•™å„å…³é”®è¯çš„é«˜é¢‘è¯
    
    ç¤ºä¾‹ï¼š
    -----------
    >>> df_word_freq = create_word_frequency_dataframe(word_freq_by_keyword, top_n=50)
    >>> print(df_word_freq.head())
    """
    print("\nåˆ›å»ºè¯é¢‘é•¿è¡¨...")
    word_freq_list = []
    
    for keyword, word_freq in word_freq_by_keyword.items():
        for idx, (word, freq) in enumerate(word_freq.most_common(top_n), 1):
            word_freq_list.append({
                'å…³é”®è¯': keyword,
                'è¯': word,
                'è¯é¢‘': freq
            })
    
    df_word_freq = pd.DataFrame(word_freq_list)
    print(f"âœ“ è¯é¢‘é•¿è¡¨åˆ›å»ºå®Œæˆï¼Œå…± {len(df_word_freq)} æ¡è®°å½•")
    
    return df_word_freq


def create_wordclouds(df, keyword_column='å…³é”®è¯', text_column='å¾®åšæ­£æ–‡',
                      word_column='è¯', freq_column='è¯é¢‘',
                      top_n=30, font_path=r"D:\code\fonts\NotoSansSC-Bold.ttf",
                      colors_list=None, cols=3, figsize=(20, 10),
                      prefer_horizontal=1.0, relative_scaling=0.5, min_font_size=10,
                      show=True):
    """
    ä¸ºæ¯ä¸ªå…³é”®è¯ç”Ÿæˆè¯äº‘å›¾ã€‚å‡½æ•°å…¼å®¹ä¸¤ç§è¾“å…¥ï¼š
    - ä¼ å…¥çš„ `df` å·²ä¸ºè¯é¢‘é•¿è¡¨ï¼ŒåŒ…å«(å…³é”®è¯, è¯, è¯é¢‘)åˆ—ï¼›
    - æˆ–è€…ä¼ å…¥åŸå§‹æ–‡æœ¬è¡¨ï¼Œå‡½æ•°ä¼šåŸºäº `text_column` å’Œ `keyword_column` è‡ªåŠ¨åˆ†è¯å¹¶ç»Ÿè®¡è¯é¢‘åå†ç»˜å›¾ã€‚

    å‚æ•°ï¼š
    -----------
    df : pd.DataFrame
        è¾“å…¥DataFrameï¼Œæ—¢å¯ä»¥æ˜¯è¯é¢‘é•¿è¡¨ä¹Ÿå¯ä»¥æ˜¯åŸå§‹æ–‡æœ¬è¡¨ã€‚
    keyword_column : str
        å…³é”®è¯åˆ—åï¼ˆå¿…é¡»ï¼‰ã€‚
    text_column : str
        åŸå§‹æ–‡æœ¬åˆ—åï¼Œä»…å½“ä¼ å…¥åŸå§‹æ–‡æœ¬è¡¨æ—¶éœ€è¦ï¼ˆé»˜è®¤ 'å¾®åšæ­£æ–‡'ï¼‰ã€‚
    word_column : str
        è¯é¢‘è¡¨ä¸­è¡¨ç¤ºè¯çš„åˆ—åï¼ˆé»˜è®¤ 'è¯'ï¼‰ã€‚
    freq_column : str
        è¯é¢‘è¡¨ä¸­è¡¨ç¤ºé¢‘ç‡çš„åˆ—åï¼ˆé»˜è®¤ 'è¯é¢‘'ï¼‰ã€‚
    top_n : int
        æ¯ä¸ªå…³é”®è¯å–å‰Nä¸ªè¯ç”¨äºç»˜å›¾ï¼ˆé»˜è®¤30ï¼‰ã€‚
    font_path : str or None
        å­—ä½“è·¯å¾„ï¼Œç”¨äºæ”¯æŒä¸­æ–‡æ˜¾ç¤ºã€‚è‹¥ä¸ºNoneåˆ™ä½¿ç”¨ç³»ç»Ÿé»˜è®¤å­—ä½“ã€‚
    colors_list : list or None
        é¢œè‰²åˆ—è¡¨ï¼Œç”¨äºæ„é€ colormapï¼ˆé»˜è®¤è“ç»¿é»„æ¸å˜ï¼‰ã€‚
    cols : int
        å­å›¾åˆ—æ•°ï¼ˆé»˜è®¤3ï¼‰ã€‚
    figsize : tuple
        å›¾åƒå¤§å°ï¼ˆå®½, é«˜ï¼‰ã€‚
    prefer_horizontal, relative_scaling, min_font_size : å‚æ•°ä¼ é€’ç»™ WordCloudã€‚
    show : bool
        æ˜¯å¦ç«‹å³è°ƒç”¨ `plt.show()` å±•ç¤ºå›¾åƒã€‚

    è¿”å›å€¼ï¼š
    -----------
    matplotlib.figure.Figure
        è¿”å›ç”Ÿæˆçš„ Figure å¯¹è±¡ï¼ˆä¾¿äºåœ¨ notebook ä¸­è¿›ä¸€æ­¥ä¿å­˜æˆ–è°ƒæ•´ï¼‰ã€‚

    è¯´æ˜ï¼š
    -----------
    - å¦‚æœä¼ å…¥çš„æ˜¯åŸå§‹æ–‡æœ¬è¡¨ï¼Œä¼šè°ƒç”¨ `tokenize_and_count_words` ç”Ÿæˆè¯é¢‘ï¼›
    - è‹¥ä¼ å…¥è¯é¢‘é•¿è¡¨ï¼Œè¯·ç¡®ä¿åŒ…å« `keyword_column`ã€`word_column` å’Œ `freq_column`ã€‚
    """

    # å‡†å¤‡é¢œè‰²æ˜ å°„
    if colors_list is None:
        colors_list = ['#208fc6', '#5AABDB', '#80d16a', '#C4DA4C', '#f9c92b']
    custom_cmap = LinearSegmentedColormap.from_list('blue_green_yellow', colors_list)

    # åˆ¤æ–­ df æ˜¯è¯é¢‘é•¿è¡¨è¿˜æ˜¯åŸå§‹æ–‡æœ¬è¡¨
    is_wordfreq_table = {keyword_column, word_column, freq_column}.issubset(set(df.columns))

    if is_wordfreq_table:
        df_word_freq = df.copy()
    else:
        # éœ€è¦åŸå§‹æ–‡æœ¬åˆ—å­˜åœ¨
        if text_column not in df.columns:
            raise ValueError(f"è¾“å…¥DataFrameæ—¢ä¸æ˜¯è¯é¢‘è¡¨ï¼Œä¹Ÿç¼ºå°‘æ–‡æœ¬åˆ—: {text_column}")
        # ä½¿ç”¨ç°æœ‰çš„åˆ†è¯ä¸ç»Ÿè®¡å‡½æ•°
        word_freq_by_keyword = tokenize_and_count_words(df, text_column=text_column, keyword_column=keyword_column)
        df_word_freq = create_word_frequency_dataframe(word_freq_by_keyword, top_n=top_n)

    # å‡†å¤‡å…³é”®è¯åˆ—è¡¨
    keywords = sorted(df_word_freq[keyword_column].unique())
    n = len(keywords)
    if n == 0:
        raise ValueError("æœªæ‰¾åˆ°ä»»ä½•å…³é”®è¯ç”¨äºç”Ÿæˆè¯äº‘")

    rows = math.ceil(n / cols)
    fig, axes = plt.subplots(rows, cols, figsize=figsize)
    # ç»Ÿä¸€å¤„ç† axes ä¸ºä¸€ç»´åˆ—è¡¨
    if isinstance(axes, plt.Axes) or axes.ndim == 0:
        axes = [axes]
    else:
        axes = axes.flatten()

    for idx, keyword in enumerate(keywords):
        ax = axes[idx]
        subset = df_word_freq[df_word_freq[keyword_column] == keyword]
        topn = subset.nlargest(top_n, freq_column)
        freqs = dict(zip(topn[word_column], topn[freq_column]))

        if not freqs:
            ax.axis('off')
            continue

        wc = WordCloud(
            font_path=font_path,
            width=400,
            height=300,
            background_color='white',
            colormap=custom_cmap,
            prefer_horizontal=prefer_horizontal,
            relative_scaling=relative_scaling,
            min_font_size=min_font_size
        ).generate_from_frequencies(freqs)

        ax.imshow(wc, interpolation='bilinear')
        ax.set_title(f'{keyword} - TOP{top_n}çƒ­è¯', fontsize=12, fontweight='bold')
        ax.axis('off')

    # å…³é—­å¤šä½™å­å›¾
    for j in range(n, len(axes)):
        try:
            axes[j].axis('off')
        except Exception:
            pass

    plt.tight_layout()
    if show:
        # show the plot but do not return the Figure object to avoid
        # Jupyter displaying it twice (once from plt.show() and once
        # from the returned Figure being auto-displayed).
        plt.show()
        return None

    # When not showing immediately, return the Figure so caller can save or display it.
    return fig
