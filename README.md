# å¾®åšå¸–å­åˆ†æå·¥å…·åŒ… | Weibo Post Analysis Toolkit

ä¸€ä¸ªç”¨äºå¤„ç†ã€åˆ†æå’ŒæŒ–æ˜å¾®åšå¸–å­æ•°æ®çš„ Python å·¥å…·åŒ…ã€‚æœ¬é¡¹ç›®æä¾›äº†æ•°æ®é¢„å¤„ç†ã€æ–‡æœ¬æ¸…æ´—ã€åˆ†è¯ã€ä»¥åŠå¤§è§„æ¨¡å¾®åšè¯­æ–™çš„ç»Ÿè®¡åˆ†æç­‰å®ç”¨åŠŸèƒ½ã€‚

*A Python toolkit for processing, analyzing, and mining Weibo post data. This project provides practical functions for data preprocessing, text cleaning, word segmentation, and large-scale corpus statistical analysis.*

## é¡¹ç›®ç®€ä»‹ | Project Overview

æœ¬å·¥å…·åŒ…å¯å°†åŸå§‹å¾®åšæ•°æ®ï¼ˆæ¨èä½¿ç”¨ [weibo-search](https://github.com/dataabc/weibo-search) æŠ“å–ï¼‰å¤„ç†ä¸ºç»“æ„åŒ–æ•°æ®é›†ï¼Œé€‚ç”¨äºè¯­è¨€å­¦ã€ç¤¾äº¤åª’ä½“å’Œæƒ…æ„Ÿåˆ†æç­‰ç ”ç©¶ã€‚

*This toolkit can process raw Weibo data (recommended to use [weibo-search](https://github.com/dataabc/weibo-search) for crawling) into structured datasets, suitable for research in linguistics, social media, and sentiment analysis.*

## åŠŸèƒ½ç‰¹æ€§ | Features

- **æ•°æ®åŠ è½½**ï¼šæ‰¹é‡åŠ è½½æ–‡ä»¶å¤¹ä¸‹çš„ CSV æ–‡ä»¶ï¼Œè‡ªåŠ¨ä»æ–‡ä»¶åæå–å…³é”®è¯
- **æ–‡æœ¬é¢„å¤„ç†**ï¼šæ¸…æ´—å’Œè§„èŒƒåŒ–å¾®åšæ–‡æœ¬ï¼ˆå»é™¤ç½‘å€ã€è¡¨æƒ…ã€ç‰¹æ®Šå­—ç¬¦ç­‰ï¼‰
- **åˆ†è¯**ï¼šåŸºäº Jieba çš„ä¸­æ–‡åˆ†è¯ï¼Œæ”¯æŒè‡ªå®šä¹‰è¯é•¿è¿‡æ»¤
- **ç»Ÿè®¡åˆ†æ**ï¼š
  - æŒ‰å…³é”®è¯/è¯é¢˜ç»Ÿè®¡è¯é¢‘
  - è¯é¢˜æå–ä¸æ’åº
  - å¸–å­å»é‡
- **å¯è§†åŒ–**ï¼šæ ¹æ®è¯é¢‘ç”Ÿæˆè¯äº‘å›¾ç‰‡

<br>

- **Data Loading**: *Batch load CSV files in a folder, automatically extract keywords from filenames*
- **Text Preprocessing**: *Clean and normalize Weibo text (remove URLs, emojis, special characters, etc.)*
- **Word Segmentation**: *Chinese word segmentation based on Jieba, supports custom word length filtering*
- **Statistical Analysis**:
  - *Count word frequency by keyword/topic*
  - *Topic extraction and ranking*
  - *Post deduplication*
- **Visualization**: *Generate word cloud images based on word frequency*

## å®‰è£…æ–¹æ³• | Installation

```bash
# å…‹éš†ä»“åº“
# Clone the repository

git clone <repo-url>
cd post_analysis

# å®‰è£…ä¾èµ–
# Install dependencies

pip install pandas jieba wordcloud matplotlib
```

## å¿«é€Ÿä¸Šæ‰‹ | Quick Start

### åŠ è½½å¾®åšå¸–å­ | Load Weibo Posts

```python
from post_analysis import load_posts_from_folder

# åŠ è½½æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰ CSV æ–‡ä»¶
# Load all CSV files in the folder
# æ–‡ä»¶åæ—  %23 å‰ç¼€ï¼šå…³é”®è¯ç›´æ¥å–è‡ªæ–‡ä»¶å
# If filename has no %23 prefix: keyword is taken directly from filename
# æ–‡ä»¶åæœ‰ %23 å‰ç¼€ï¼šå¦‚ %23å…³é”®è¯%23 â†’ å…³é”®è¯æå–ä¸º"å…³é”®è¯"
# If filename has %23 prefix: e.g. %23keyword%23 â†’ keyword extracted as "keyword"
data = load_posts_from_folder('./posts')

print(data.head())
print(data['å…³é”®è¯'].unique())  # æŸ¥çœ‹æ‰€æœ‰å…³é”®è¯ / View all keywords
```

### æ–‡æœ¬åˆ†æ | Text Analysis

```python
from post_analysis import clean_text, tokenize_and_count_words, create_word_frequency_dataframe

# æ–‡æœ¬æ¸…æ´—
# Text cleaning
cleaned_text = clean_text("è¿™æ˜¯å¾®åšæ–‡æœ¬ http://example.com ğŸ˜Š")

# æŒ‰å…³é”®è¯åˆ†è¯å¹¶ç»Ÿè®¡è¯é¢‘
# Tokenize by keyword and count word frequency
word_freq_by_keyword = tokenize_and_count_words(
  data,
  text_column='å¾®åšæ­£æ–‡',
  keyword_column='å…³é”®è¯',
  word_length_range=(2, 4)
)

# ç”Ÿæˆè¯é¢‘ DataFrame
# Generate word frequency DataFrame
freq_df = create_word_frequency_dataframe(word_freq_by_keyword)
```

### ç”Ÿæˆè¯äº‘ | Generate Word Cloud

```python
from post_analysis import create_wordclouds

# ä¸ºæ¯ä¸ªå…³é”®è¯ç”Ÿæˆå¹¶ä¿å­˜è¯äº‘å›¾ç‰‡
# Generate and save word cloud images for each keyword
create_wordclouds(word_freq_by_keyword, output_dir='./wordclouds')
```

### è¯é¢˜æå– | Topic Extraction

```python
from post_analysis import extract_top_topics

# ä»å¸–å­ä¸­æå–å‰3ä¸ªçƒ­é—¨è¯é¢˜
# Extract top 3 hot topics from posts
data_with_topics = extract_top_topics(data, topics_column='è¯é¢˜')
```

## æ–‡ä»¶å‘½åè§„èŒƒ | File Naming Convention

æŠ“å–å¤šå…³é”®è¯å¸–å­æ—¶ï¼Œå·¥å…·æ”¯æŒ URL ç¼–ç çš„æ–‡ä»¶åï¼š

- **æ ‡å‡†æ ¼å¼**ï¼š`keyword.csv` â†’ å…³é”®è¯ä¸ºâ€œkeywordâ€
- **URL ç¼–ç æ ¼å¼**ï¼š`%23keyword%23.csv` â†’ å…³é”®è¯ä¸ºâ€œkeywordâ€

*When crawling posts with multiple keywords, the tool supports URL-encoded filenames:*

- **Standard format**: *`keyword.csv` â†’ keyword is "keyword"*
- **URL-encoded format**: *`%23keyword%23.csv` â†’ keyword is "keyword"*

åŠ è½½å™¨ä¼šè‡ªåŠ¨è¯†åˆ«å¹¶å¤„ç†ä¸Šè¿°ä¸¤ç§æ ¼å¼ã€‚

*The loader will automatically recognize and handle both formats above.*

## æ•°æ®æ ¼å¼ | Data Format

è¾“å…¥çš„ CSV æ–‡ä»¶åº”åŒ…å«å¾®åšæ•°æ®çš„æ ‡å‡†å­—æ®µï¼Œä¾‹å¦‚ï¼š
- `å¾®åšæ­£æ–‡`ï¼ˆå¸–å­å†…å®¹ï¼‰
- `è¯é¢˜`ï¼ˆè¯é¢˜/æ ‡ç­¾ï¼‰
- `id`ï¼ˆå¸–å­IDï¼‰
- å…¶ä»–å…ƒæ•°æ®å­—æ®µ

*The input CSV file should contain standard Weibo data fields, such as:*
- *`å¾®åšæ­£æ–‡` (post content)*
- *`è¯é¢˜` (topic/tag)*
- *`id` (post ID)*
- *other metadata fields*

## å»é‡ | Deduplication

å»é™¤é‡å¤å¸–å­ï¼ŒåŒæ—¶ä¿ç•™å…³é”®è¯å…³è”ï¼š

*Remove duplicate posts while retaining keyword associations:*

```python
from post_analysis import dedupe_posts

dedupe_df = dedupe_posts(data, id_column='id', keyword_column='å…³é”®è¯')
```

## æ¨¡å—ç»“æ„ | Module Structure

- **`pre_processing.py`**ï¼šæ•°æ®åŠ è½½ã€è¯é¢˜æå–ä¸å»é‡
- **`corpus_analysis.py`**ï¼šæ–‡æœ¬æ¸…æ´—ã€åˆ†è¯ã€è¯é¢‘åˆ†æä¸å¯è§†åŒ–
- **`__init__.py`**ï¼šåŒ…å¯¼å‡ºä¸ç‰ˆæœ¬ä¿¡æ¯

<br>

- **`pre_processing.py`**: *Data loading, topic extraction, and deduplication*
- **`corpus_analysis.py`**: *Text cleaning, word segmentation, word frequency analysis, and visualization*
- **`__init__.py`**: *Package export and version info*

## ä¾èµ–è¦æ±‚ | Requirements

- Python 3.7+
- pandas
- jieba
- wordcloud
- matplotlib

- *Python 3.7+*
- *pandas*
- *jieba*
- *wordcloud*
- *matplotlib*

## æ•°æ®æ¥æº | Data Source

åŸå§‹å¾®åšæ•°æ®æ¨èä½¿ç”¨ [weibo-search](https://github.com/dataabc/weibo-search) å·¥å…·æŠ“å–ã€‚

*It is recommended to use the [weibo-search](https://github.com/dataabc/weibo-search) tool to crawl raw Weibo data.*

## è®¸å¯è¯ | License

MIT License

*MIT License*

## è´¡çŒ® | Contributing

æ¬¢è¿æäº¤ issue æˆ– pull request å‚ä¸è´¡çŒ®ï¼

*Contributions are welcome via issues or pull requests!*